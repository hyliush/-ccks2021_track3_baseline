{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-65f7bf991eab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcolab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcolab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive/MyDrive/Tianchi_MATCH'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "colab = True\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.chdir('/content/drive/MyDrive/Tianchi_MATCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if colab:\n",
    "    !pip install transformers\n",
    "    !pip install fastNLP\n",
    "    !pip install fitlog\n",
    "    !pio install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler,SequentialSampler\n",
    "from transformers import BertConfig,BertTokenizer,BertForSequenceClassification,BertModel\n",
    "\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from pipeline import Trainer,Tester\n",
    "from dataset import DataSet\n",
    "from model import PointwiseMatching\n",
    "import torch\n",
    "import sys\n",
    "from torchmetrics import F1,Recall,Precision,Accuracy\n",
    "from config import Config\n",
    "from fastNLP import logger\n",
    "from datetime import datetime\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "debug = True\n",
    "colab = False\n",
    "if colab:\n",
    "    sys.argv =['name',\n",
    "            '--model_name_or_path', '../pretrained_model/chinese_roberta_wwm_ext_pytorch',\n",
    "            '--data_dir', '../data/tianchi',\n",
    "            #'--do_test',\n",
    "            '--do_train',\n",
    "            '--save_logs',\n",
    "            '--n_epochs', '3',\n",
    "            '--num_classification', '3',\n",
    "            '--max_seq_length', '80',\n",
    "            '--train_batch_size', '16',\n",
    "            '--eval_batch_size', '16',\n",
    "            '--task', '0', '1',\n",
    "            '--early_stop', '6',\n",
    "            '--update_every', '2',\n",
    "            '--validate_every', '1000',\n",
    "            '--print_every', '4',\n",
    "            '--warmup_steps', '0',\n",
    "            '--warmup_proportion', '0.1',\n",
    "            '--learning_rate', '2e-5',\n",
    "            '--adam_epsilon', '1e-6',\n",
    "            '--weight_decay', '1e-4']\n",
    "\n",
    "    parser = Config.get_parser()\n",
    "    args = parser.parse_args()\n",
    "else:\n",
    "    args = Config.get_default_cofig()\n",
    "\n",
    "if debug:\n",
    "    args.save_runs = True\n",
    "    args.save_logs = True\n",
    "    args.validate_every = 50\n",
    "\n",
    "cache_dir = os.path.join(args.data_dir,'cache')\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.makedirs(cache_dir)\n",
    "\n",
    "output_dir = os.path.join(args.data_dir,'model')\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if args.save_logs:\n",
    "    # logs_dir\n",
    "    logs_dir = os.path.join(output_dir,'logs')\n",
    "    if not os.path.exists(logs_dir):\n",
    "        os.makedirs(logs_dir)\n",
    "    logger.add_file(os.path.join(logs_dir,'{}'.format(str(datetime.now().strftime('%Y-%m-%d-%H-%M-%S')))),level='info')\n",
    "\n",
    "if args.save_runs:\n",
    "    # runs_dir\n",
    "    runs_dir = os.path.join(output_dir,'runs')\n",
    "    writer = SummaryWriter(log_dir = os.path.join(runs_dir,'{}'.format(str(datetime.now().strftime('%Y-%m-%d-%H-%M-%S')))))\n",
    "else:\n",
    "    writer = None\n",
    "\n",
    "# prepare model\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_name_or_path, do_lower_case=args.do_lower_case)\n",
    "config = BertConfig.from_pretrained(args.model_name_or_path, num_labels=args.num_classification)\n",
    "# model = PointwiseMatching.from_pretrained(args.model_name_or_path,config = config)\n",
    "model = BertForSequenceClassification.from_pretrained(args.model_name_or_path, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "\n",
    "# prepare train_dataloader\n",
    "dt = DataSet(tokenizer=tokenizer)\n",
    "train_name='train.txt'\n",
    "dev_name = 'dev.txt'\n",
    "train_cache_name = os.path.join(cache_dir,'train_batchsize{}'.format(args.train_batch_size))\n",
    "train_dataloader = dt.prepare_dataloader(file_path=os.path.join(args.data_dir, train_name),\n",
    "                                            batch_size=args.train_batch_size,\n",
    "                                            max_seq_length=args.max_seq_length,\n",
    "                                            sampler=RandomSampler,\n",
    "                                            _cache_fp=train_cache_name, _refresh=False)\n",
    "# need to revised for specific model\n",
    "# if args.save_runs:\n",
    "#     writer.add_graph(model.to('cpu'),[i[0].unsqueeze(0) if i.shape else i for i in (*next(iter(train_dataloader))[:3],torch.tensor(True))])\n",
    "\n",
    "# prepare scheduler\n",
    "if args.warmup_proportion>0:\n",
    "    n_steps = len(train_dataloader)*args.n_epochs/args.update_every\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(args.warmup_proportion*n_steps),\n",
    "                                num_training_steps=n_steps)\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# prepare dev_dataloader\n",
    "dev_cache_name = os.path.join(cache_dir,'dev_batchsize{}'.format(args.eval_batch_size))\n",
    "dev_dataloader = dt.prepare_dataloader(file_path=os.path.join(args.data_dir, dev_name),\n",
    "                                        batch_size=args.eval_batch_size,\n",
    "                                        max_seq_length=args.max_seq_length,\n",
    "                                        sampler=SequentialSampler,\n",
    "                                       _cache_fp=dev_cache_name, _refresh=False)\n",
    "\n",
    "# prepare metrics\n",
    "f1 = F1(average='macro',num_classes=args.num_classification)\n",
    "recall = Recall(average='macro',num_classes=args.num_classification)\n",
    "precision = Precision(average='macro',num_classes=args.num_classification)\n",
    "acc = Accuracy(num_classes=args.num_classification)\n",
    "metrics = {'f1':f1,'recall':recall,'precision':precision,'acc':acc}\n",
    "\n",
    "def generate_submit(predictor,read_filename='Xeon3NLP_round1_test_20210524.txt',write_filename='submit_addr_match_runid.txt'):\n",
    "    # prepare test_dataloder\n",
    "    #if os.path.isfile(os.path.join(args.data_dir,write_filename)):\n",
    "    #    raise FileExistsError('write_file has existed')\n",
    "\n",
    "    fo = open(os.path.join(args.data_dir,write_filename), \"w\",encoding='utf8')\n",
    "    import json\n",
    "    f = open(os.path.join(args.data_dir,read_filename), 'r',encoding='utf8')\n",
    "    for line in tqdm(f.readlines(),total=len(f.readlines()),desc='generate submit file'):\n",
    "        t = json.loads(line)\n",
    "        for j in range(len(t['candidate'])):\n",
    "            l = dict()\n",
    "            l['text_a'] = t['query']\n",
    "            l['text_b'] = t['candidate'][j]['text']\n",
    "            l['label'] = 0\n",
    "            dataloader = dt.prepare_dataloader_from_iterator([l],\n",
    "                                                             args.eval_batch_size,\n",
    "                                                             args.max_seq_length,\n",
    "                                                             sampler=SequentialSampler)\n",
    "            result = predictor.predict(dataloader)\n",
    "            idx = result['infer_labels']\n",
    "            if idx[0] == 0:\n",
    "                t['candidate'][j]['label'] = '不匹配'\n",
    "            elif idx[0] == 1:\n",
    "                t['candidate'][j]['label'] = '部分匹配'\n",
    "            else:\n",
    "                t['candidate'][j]['label'] = '完全匹配'\n",
    "        fo.write(json.dumps(t, ensure_ascii=False))\n",
    "        fo.write('\\n')\n",
    "    f.close()\n",
    "    fo.close()\n",
    "\n",
    "args.do_train = True\n",
    "dt = DataSet(tokenizer=tokenizer,verbose=0,use_tqdm=False)\n",
    "if args.do_train:\n",
    "    load_model = False\n",
    "    if load_model:\n",
    "        model_path = os.path.join(output_dir, 'best_BertForSequenceClassification_2021-06-18-17-09-32_')\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(\"folder `{}` does not exist. Please make sure model are there.\".format(model_path))\n",
    "        states = torch.load(model_path).state_dict()\n",
    "        model.load_state_dict(states)\n",
    "\n",
    "    # trainer\n",
    "    trainer = Trainer(train_dataloader, model, optimizer, scheduler=scheduler,\n",
    "                update_every=args.update_every,n_epochs=args.n_epochs,\n",
    "                 print_every=args.print_every,early_stop=args.early_stop,metrics=metrics,\n",
    "                 dev_dataloader=dev_dataloader, validate_every=args.validate_every,\n",
    "                save_path=output_dir,customize_model_name = None,seed=args.seed,debug=debug,\n",
    "                writer=writer)\n",
    "\n",
    "    trainer.train()\n",
    "    # dev\n",
    "    predictor = Tester(model)\n",
    "    # result = predictor.predict(dev_dataloader)\n",
    "    generate_submit(predictor,'Xeon3NLP_round1_train_20210524.txt','submit_dev.txt')\n",
    "    generate_submit(predictor, read_filename='Xeon3NLP_round1_test_20210524.txt',\n",
    "                    write_filename='submit_addr_match_runid.txt')\n",
    "\n",
    "#args.do_test = True\n",
    "if args.do_test:\n",
    "    # prepare predictor\n",
    "    model_path = os.path.join(output_dir, 'best_BertForSequenceClassification_2021-06-21-12-08-52_')\n",
    "    states = torch.load(model_path).state_dict()\n",
    "    model.load_state_dict(states)\n",
    "    predictor = Tester(model)\n",
    "    generate_submit(predictor,read_filename='Xeon3NLP_round1_test_20210524.txt',write_filename='submit_addr_match_runid.txt')\n",
    "\n",
    "\n",
    "    # if args.predict_text:\n",
    "    #     def prepare_data(text):\n",
    "    #         return [{'_id':'uts1299034-124', 'text_a':text, 'label':0}]\n",
    "    #\n",
    "    #     dataloader = dt.prepare_dataloader_from_iterator(prepare_data(args.predict_text),\n",
    "    #                                                      args.eval_batch_size,\n",
    "    #                                                      args.max_seq_length,\n",
    "    #                                                      sampler=SequentialSampler)\n",
    "    # if args.predict_filename:\n",
    "    #     dataloader = dt.prepare_dataloader(os.path.join(args.data_dir, args.predict_filename),\n",
    "    #                                         args.eval_batch_size,\n",
    "    #                                         args.max_seq_length,\n",
    "    #                                         sampler=SequentialSampler)\n",
    "    #\n",
    "    # result = predictor.predict(dataloader)\n",
    "    # logits = result['infer_logits']\n",
    "    #\n",
    "    # df=pd.read_csv(os.path.join(args.data_dir, args.predict_filename))\n",
    "    # for i in range(args.num_classification):\n",
    "    #     df['label_{}'.format(str(i))]=logits[:,i]\n",
    "    # df[['_id']+['label_{}'.format(str(i)) for i in range(args.num_classification)]].to_csv(\n",
    "    #     os.path.join(args.output_dir, \"sub.csv\"),index=False)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
